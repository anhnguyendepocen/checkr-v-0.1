---
title: "Checking Code Answers"
author: "Daniel Kaplan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r include = FALSE}
library(ggplot2)
library(dplyr)
library(checkr)
library(magrittr)
```

To help meet the demand for R-related training, various *interactive tutorial systems* have been developed. These include [Swirl](http://swirlstats.com/), [DataCamp.com](http://DataCamp.com), and the Shiny-based system introduced by RStudio: [`tutor`](http://github.com/rstudio/tutor). In each of these systems, a problem is posed to the student, who has an opportunity to construct R commands as a solution. The R commands are then passed to an evaluation system.

The `checkr` package provides such an evaluation system that provides both evaluative and formative feedback and, at the instructor's discretion, logs student submissions for assigning credit and to support looking for common patterns of misconception in student answers.

"Evaluative feedback," in the jargon of education, means telling whether the student was right or wrong; whether the submitted work satisfied the requirements. In computer programming, evaluative feedback can be based on whether a correct result was computed, whether a function works as required, and so on. `Checkr` provides simple mechanisms for comparing the quantities computed by the student submission to known values.

"Formative feedback" attempts to help the student develop a better submission. This is a more subtle situation, since good feedback must involve understanding of how student's think about the problem, typical patterns in student answers, and the variety of ways that the problem can be approached. `Checkr` provides facilities for looking within the student's code for functions and their arguments, for checking intermediate results, and to look for specific mistake patterns.

Readers may also want to check out the system for *[submission correctness tests](https://www.datacamp.com/community/blog/submission-correctness-testing-in-datacamp-datacamp-part-i#gs.vuTG8Xw)* provided by DataCamp.


## A trivial example: 2 + 2

As an illustration of the need for an evaluation system and how `checkr` statements are writen, consider this trivial problem: 

> Write the code to add two and two. 

Of course, we expect the student to submit `2 + 2` as the response. But they might have

* submitted nothing
* submitted something like `3 + 1`
* submitted something like `2 / 2`

`Checkr` allows you to write test statements that

1. allow the values produced by the code to be tested against a specified solution and 
2. examine the student's submission to make sure the code was of the specified form.

A tutorial system will typically collect the student code submissions as text, and may evaluate the code in order to verify that it runs.  Here are a few different possible submissions for the `2 + 2` problem:

```r
2 + 2     # correct
2 / 2     # right arguments but wrong function
3 + 1     # right function but wrong arguments
`+`(2, 2) # correct
(2 + 2)   # correct
```

Note that it's not sufficient merely to check that the result of the calculation is the value 4. Submission three produces a value of 4 but is an incorrect answer to the stated problem. So in addition to checking the result, aspects of the way the result was calculated should also be checked. Here, that means perhaps these three things:

1. That `+` was used appropriately.
2. That the first argument to `+` is 2.
3. That is second argument to `+` is 2. 

Here are some tests from `checkr` that are relevant. Each test takes the form of a pattern and a message to return if that pattern is not found.
```{r}
library(checkr)
test_1 <- find_call("whatever + whatever", "you need to use addition (+)")
test_2 <- find_call("2 + whatever", "the first argument should be 2")
test_3 <- find_call("whatever + 2", "the second argument should be 2")
test_4 <- check_value(agrees(x == 4), "the result should be 4")
```

The first test makes sure that `+` has been used with two arguments, but disregards the values of the arguments. The second test checks that 2 is the first argument. The third test checks that 3 is the second argument. The messages for each test can be focussed as much as desired because the individual tests each look for a specific aspect of the calculation.


# Sequences of tests

The four tests given in the `2 + 2` example are not independent of each other. For instance, `test_2` will fail for a submission like `2 * 2`, but the resulting feedback message would not be helpful. To provide meaningful formative feedback, `test_2` has to follow `test_1`. 

`Checkr` tests can be formed into sequences, so that a failure of an early test prevents the triggering of later tests. The `find_` functions in `checkr` examine the submitted code for a pattern. If that pattern is not found, the test fails. The `find_call()` function knows about functions and arguments. The string `"whatever + whatever"` in `test_1` specifies that the function `+` should be called with two arguments, although the values of those arguments can be anything at all.

The `check_` functions in `checkr` test whether a output value or argument match specified conditions. For the `2 + 2` problem, an appropriate test sequence would be

```r
USER_CODE %>% test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4
```

The `final_` directive means to feed the last calculated value from the `USER_CODE` into the following test.

Ordinarily, these test statements would be used in conjunction with the interactive tutorial system. A later section of this document will how how to set up the "two plus two" problem using `tutor`. But first, let's examine how the tests perform on the several possible submissions listed above. Since the `USER_CODE` provided by `tutor`/`checkr` will not be available in this static document, I'll use `capture.code()` to mimic the capture and transmission of code from `tutor` to `checkr`.

```{r echo = FALSE}
print.test_result <- print.capture <- function(test_output) {
  if (test_output$passed) cat("Passed!\n")
  else cat(paste("Sorry, but", test_output$message), "\n")
  return(invisible(test_output))
}
```

The obvious correct answer passes the tests.
```{r}
capture.code("2 + 2") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
```
Also passing are two variant correct forms:
```{r}
capture.code("`+`(2, 2)") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
capture.code("(2 + 2)") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
```

The incorrect submissions lead to the generation of a formative feedback message:

```{r}
capture.code("3 + 1") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
capture.code("2 / 2") %>%
  test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4 
```

# Using `checkr` with `learnr`

The documentation for the `learnr` package has instructions and examples for creating exercises. In a nutshell, you create an ordinary Rmd file with code chunks labelled to identify the exercise and the various components of the exercise:

a. The exercise itself in a chunk with a unique label. This label will identify the exercise. It's best if it is globally unique. For this example, I'll use `2016-01-04-b7f4e`.
b. [optional] the `-solution` chunk. In this example the full label will be `2016-02-04-b7f4e-solution`. This chunk contains code that solves the problem.
c. [optional] the `check` chunk that contains your problem checking statements.
d. [optional] the `code-check` chunk containing checking statements to be applied to the student's code *before* it is evaluated.
e. [optional] the `-hint` chunk containing a hint that the student can call up while working on the problem.

The document should start with a `setup` chunk, which will minimally look like the following but may also include other `library()` statements and other R code.

The third line in the chunk tells `learnr` to use the `checkr` system. Use it exactly as shown below. 

    ```{r setup, include=FALSE}
    library(learnr)
    library(checkr)
    knitr::opts_chunk$set(exercise.checker = checkr::checkr_tutor)
    ```
Following the `setup` chunk, you write freeform Rmd that includes appropriately named chunks for the exercise(s) you want to include in the file. For instance, the markdown for the "two plus two" exercise could look like this:

    Write a statement to calculate two plus two.
    
    ```{r, 2016-02-04-b7f4e, exercise = TRUE}
    ____ + ____
    ```
    
You can put any text you deem appropriate inside the chunk. Here, for this example, I've put a little hint: "____ + _____"; this is a pedagogical choice that you make when designing the exercise. You can include any text you like, even if it is not valid R. This chunk will create an exercise "code pane" that students can edit and evaluate. 


When the student presses the "Submit Answer" button in the `tutor` code pane, the student's code is sent off for checking. There are two kinds of check:

1. Checking of the text of the student's submission, without evaluating it. Any such checks are to be put in the `-code-check` chunk for the problem. For instance, `____ + ____` is not valid R syntax (variables can't start with `_`). By default, the code checks are nil. That is, by default there is no pre-evaluation checking of the code. Nonetheless, every bit of submitted code must pass the usual execution-time checks provided by the R interpreter.

There's no strict need to include pre-evaluation checking, but it can make the tutorial friendlier for neophytes. For this exercise, which has some blanks in the text pre-positioned in the code pane, an pre-evaluation check like this would be appropriate.

    ```{r 2016-02-04-b7f4e-check, echo=FALSE}
    check_blanks(USER_CODE)
    ```

2. Checking of the evaluatable text in the code pane. Once the code pane text passes the pre-evaluation check, it is subjected to the tests given in the `-check` chunk. Since the pre-evaluation check always includes the R interpreter, any code that reaches the `-check` chunk must have been valid, executable R. You can write your check statements with this in mind.

The `-check` chunk contains R code. In principle, any R code can be included. In practice, the chunk will include functions from the `checkr` package. The checking code is run in an environment that includes a object called `USER_CODE`, which is the text of the code pane as well as the results of evaluating the code. If you included an optional `-solution` chunk, there will be a similar object based on that code, named `SOLN_CODE`. These two objects are used as inputs to the `checkr` tests.

    ``{r 2016-02-04-b7f4e-check, echo=FALSE}
    test_1 <- find_call("whatever + whatever",  
                message = "need to use addition (+)")
    test_2 <- find_call("2 + whatever", 
                message = "first argument should be 2")
    test_3 <- find_call("whatever + 2", 
                message = "second argument should be 2")
    test_4 <- check_value(agrees(x == 4), 
                message = "the result should be 4")
    USER_CODE %>% test_1 %>% test_2 %>% test_3 %>% final_ %>% test_4
    ```

# A Starting Template

It's easy to make mistakes with the names of exercise chunks. To simplify things, `checkr` provides a `new_exercise()` function. This generates a template exercise file with a (probably) unique name and saves it in the current working directory (use `getwd()` to see what this is and `setwd()` to change it *before* calling `new_exercise()`.) The template is a complete working exercise with all the related chunks given consistent names.

`New_exercise()` takes two character-string arguments that are used to form the unique name for the exercise. The first, `collection` is the starting bit for the name. You can set this to whatever you like. I recommend that you use a short string of letters that identifies the project of which the exercise forms a part. For instance, for my *[Data Computing](http://project-mosaic-books.com/?page_id=16)* book, I set `collection` to `DC`. The second argument `unique_id` is there in case you want to create your own ID for the exercise. If it is omitted, a unique ID will be provided for you in the form of a date followed by a five number random hex code. For example: `DC-2017-01-05-496c5`.

The file created by `new_exercise()` contains properly named `learnr` chunks for one exercise. I develop and test the exercise using just that file. But for deploying the exercise, I create another `learnr` file that includes single-exercise files as child documents. That file looks like this:

    ---
    title: "Data Computing: Chapter 4"
    output: 
    learnr::tutorial: default
    html_document: 
    number_sections: yes
    toc: yes
    runtime: shiny_prerendered
    ---
    
    ```{r include=FALSE}
    library(learnr)
    library(checkr)
    knitr::opts_chunk$set(exercise.checker = checkr::checkr_tutor)
    ```
    
    ```{r child = checkr::authentication(FALSE)}
    ```

    # Reading data from files

    ```{r child="Chapter-04/DC-2017-01-01-f13ea.Rmd"}
    ```

    # Data from R packages
    ```{r child="Chapter-04/DC-2017-01-01-d386f.Rmd"}
    ```

    <!-- and so on -->

I am hoping that a community of instructors will share exercises for interactive tutorials. This simple system of one exercise per file may facilitate that.

In deploying `learnr` exercises in this one-per-file manner, make sure that both the parent document and all the child documents are installed on the server. 

Note that `learnr` has a template for interactive tutorial documents that contains multiple exercises. You may well prefer to work with that format



# Framework for testing

`Checkr` examines submitted code one command line at a time. A command line is a complete piece of R code that generates a value, such as would be given in the console and evaluated when "return" is pressed. Here are some examples of statements:

* `x <- 7`
* `library(ggplot2)`
* `lm(mpg ~ hp, data = mtcars)`

Note that assignment of a name to a value, e.g. `x <- 7`, is part of the statement. 

At present, `checkr` tests are designed around command lines like the above.
When `magrittr` pipes are used, all the commands piped together constitute one statement. `checkr` will break such chains into individual statements, each having the contents between successive pipes.

Note that when curly braces are included in code, a command line is more inclusive than many people would think at first glance. For instance, `for` loops constitute a single a command line. The following chunk has only one command line: 

```r
for (k in 1:length(vec)) { 
  x <- cos(vec[k])
  sum <- sum + x
}
```

The same is true for `if` statements, such as:

```r
if (x > 0) {
  y <- cos(x)
} else {
  z <- sin(x)
}
```

Function definition with `function` also involves a single statement:

```r
funnel <- function(x, width=3) {
  ifelse(abs(x) > width, abs(x), -width)
}
```

In the future, we anticipate providing `checkr` tests that work with such constructions.


# Location tests

The location tests currently provided by `checkr` are:

* `find_assignment("var")` looks for a statement which involves assignment to the variable named in the argument (`"var"` in this example). It can also take a regex. 
* `find_statement()` looks for a character string or regex match to the body of a statement.
* `find_value(1:10)` looks for a match to the value produced by a statement. Note that the argument is unquoted.
* `similar_names("sqrt(abs(x))")` looks for a statement that has similar names (function names, variables) to those contained in the statement provided as an argument
* `final()` identifies the final statement. No argument necessary
* `find_call("cos(x + whatever)")` looks for a statement containing a function call that matches the one provided as an argument. 
* `find_names(sqrt(3 * x + y))` will look for a statement that contains function and object names matching those in the argument. Note that the argument is unquoted.
* `find_constants(3, "skippidee")` looks for a statement with numbers and character strings matching those given in the arguments. You can include as many constants as you like.
* `find_formula()` looks for a statement containing a formula, e.g. as in `lm()`.

Each passing test records the particular command line that matched the pattern. Subsequent tests can be directed to look for matches before, after, or within the command line that matched the previous test. See the location-qualifier functions \code{then}, \code{previously}, and \code{inside}. Of course, you need not use a location qualifier, in which case subsequent tests will look at *all* command lines.

There are location tests that allow you to test a value generated by the code. The simplest is \code{check_value}, which looks at the value of the whole command line. A previous locator test should have identified the particular line whose value is to be tested.  To start the example in this static vignette, here is a statement that mimics `tutor` handing off some submitted code to `checkr`.

```{r}
# this line isn't needed in an actual tutor document
USER_CODE <- capture.code("x <- 7 + 3\n sin(x)")
```

And here is what a tutor `-check` chunk using `check_value()` might look like:
```{r}
test_1 <- find_call("sin(whatever)", "you didn't use sin().")
test_2 <- check_value(match_number(sin(10), 
                                   "something's wrong with the sin() line."))
USER_CODE %>%
  test_1 %>% test_2 
```
  
Another kind of value-checking location test looks for the value of an argument to a function. The location specifier must include a \code{grab_this} to identify the particular argument to be captured. For instance:

```{r}
test_a <- find_call("whatever + whatever", "remember to use +") # regular location test
test_b <- check_argument("grab_this + whatever", 
                         match_number(17, tol = 0.1))
USER_CODE %>%
  test_a %>% test_b
```

## Combining tests

Combining two tests means to use the output of one test as the intput to another. Here are functions that combine tests in various ways. All of them produce a function as output. This output function takes a capture object as input to carry out the test.


* `any_test(t1, t2, ...)` checks whether any of the tests pass. If so, the overall result is a pass. Stops when it first finds a passing match.
* `all_tests(t1, t2, ...)` requires all the tests to pass. This accomplishes the same thing as combining the tests with `%>%`. Stops when it first encounters a failing match. 
* `branch_test(condition, yes_test, no_test)` evaluate the condition test on the capture object input. If it passes, then the result of the overall test will be the result of running the yes-test on the capture object input. Otherwise, the result of the no-test on the capture object will be the output.

## Common student mistakes

If you know about a common sort of student mistake, you may want to test specifically for that in order to give a helpful failure message. To do this, construct a test for the anticipated mistake, following it with `was_mistake()`. For example:

> What is the $x$-coordinate of a point on the unit circle at $\pi/2$?

```{r}
USER_CODE <- capture.code("sin(pi / 2)") 
test_1 <- find_call("pi/2", "you need to compute pi / 2.")
test_2 <- find_call("sin()") # wrong, but common mistake
test_3 <- find_call("cos()")
USER_CODE %>% 
  test_1 %>% test_2 %>% 
  was_mistake(message = "the x-coordinate is given by the cosine function") %>% 
  test_3 %>% 
  final_ %>% check_value(match_number(cos(pi/2)))() 
```

## Example: Testing assignment

Consider a problem like this:

> Assign the name `x` to the results of the calculation $\sqrt{7}$.

There are several things to be checked here:

1. That a name was assigned to an object.
2. That the name was `x`
3. That `sqrt()` was called.
4. That 7 was the argument to `sqrt()`.

It's the author's choice whether to test all these things. Here are some locator test sequences that an author might choose among.

```{r}
USER_CODE <- capture.code("xx <- sin(7)") # wrong in so many ways!
test_1 <- find_call("sqrt(whatever)", "use the sqrt() function.")
test_2 <- find_assignment("x")
test_3 <- check_argument("sqrt(grab_this)", match_number(7))
```

Which of these tests should come first? It depends on the points the author wants to emphasize. For example, she might want to know first whether assignment  was used.
```{r}
USER_CODE %>% test_2 %>% test_1 %>% test_3 
```
Or, the author might want to check first that the `sqrt()` function was used.
```{r}
USER_CODE %>% test_1 %>% test_3 %>% test_2 
```

## Example

Suppose the stated problem is to create some graphics showing the relationship between horsepower and gasoline mileage in the `mtcars` data.

Here's a reasonable student submission that creates a `ggplot` object.

```{r}
USER_CODE <- capture.code("
ggplot(mtcars, aes(y = mpg, x = hp)) + 
    geom_point()" 
)
```

A simple test for this is:
```{r}
test_1 <- find_value(match_class("ggplot"))
USER_CODE %>% test_1
```

But if we were looking for a `lattice` graphics object, the submission fails:
```{r eval = FALSE}
test_2 <- find_value(match_class("lattice"))
USER_CODE %>% test_2 
```

If either would suffice ...
```{r}
test_3 <- any_test(test_1, test_2)
USER_CODE %>% test_3 
```

The above example gives almost no formative feedback. By creating tests that look for narrow conditions, the feedback can be tailored to match the mistake.

Let's look at these possible submissions:

```{r}
submission_1 <- "ggplot(mtcars, aes(x = mpg, y = hp)) + geom_point()" # Wrong!
submission_2 <- "ggplot(mtcars, aes(x = hp, y = mpg)) + geom_line()" # Wrong!
submission_3 <- "ggplot(mtcars, aes(y = mpg, x = hp)) + geom_point()" # right
submission_4 <- "ggplot(mtcars) + geom_point(aes(y = mpg, x = hp))" # also right
submission_5 <- c(
"my_cars <- mtcars",
"ggplot(my_cars, aes(y = mpg, x = hp)) + geom_point()")
submission_6 <- c(
"my_cars <- mtcars %>% select(mpg, hp)",
"ggplot(my_cars, aes(y = mpg, x = hp)) + geom_point()")
```

Submission 1 has the roles of the variables `mpg` and `hp` reversed from the problem statement. Submission 2 uses `geom_line()` instead of the required `geom_point()`. Submissions 3 and 4 will all create the specified plot, but each is arranged in a different way. 

Here are some tests. Remember, the second argument is the message to return if the test *fails*.
```{r}
library(ggplot2)
test_1 <- find_call("aes(x = hp, y = whatever)", "variable 'hp' goes on the x axis")
test_2 <- find_call("aes(y = mpg, x = whatever)", "variable 'mpg' goes on the y axis")
test_3 <- find_call("geom_point()", "include a 'geom_point()' layer")
test_4 <- find_statement("mtcars") 
test_5 <- find_call("ggplot(data = whatever)", "no data handed to ggplot()")
test_6 <- check_argument("ggplot(data = grab_this)", test = match_class("data.frame"))
test_7 <- check_argument("ggplot(data = grab_this)", match_data_frame(mtcars))
```

We'll chain these seven tests together and see the results for the different submissions.

```{r}
capture.code(submission_1) %>% 
  test_1 %>% test_2 %>% test_3 %>% test_4 %>% test_5 %>% test_6 %>% test_7 
capture.code(submission_2) %>%
  test_1 %>% test_2 %>% test_3 %>% test_4 %>% test_5 %>% test_6 %>% test_7 
capture.code(submission_3) %>% 
  test_1 %>% test_2 %>% test_3 %>% test_4 %>% test_5 %>% test_6 %>% test_7 
capture.code(submission_4) %>%
  test_1 %>% test_2 %>% test_3 %>% test_4 %>% test_5 %>% test_6 %>% test_7 
```

## Setting the success message

When the tests pass, you may want to provide a custom message to the student. For instance, you might want to comment on some aspect of the result, e.g. "$R^2$ always goes up when you add a new term to a model."

You can set the message with `set_success_message()` in the `-check` chunk.

## Checking the value itself

`Checkr` includes a variety of functions for checking whether a value produced by a command meets specified criteria.  For example, imagine this exercise statement:

> Create a sequence of odd numbers from 11 to 31 (inclusive) using the colon operator.

Here are a few reasonable student submissions:
```{r}
submission_1 <- capture.code("seq(11, 31, by = 2)") # right value, but not what was asked
submission_2 <- capture.code("11 + 2*(0:10)") # right
submission_3 <- capture.code("11 + 2*(1:11)") # uses colon, but wrong result
```

In checking the submission, you want to make sure that the colon operator was used and that the final result is what was asked for.

```{r}
test_1 <- find_call("whatever : whatever", "you didn't use the colon operator")
test_2 <- check_value(match_vector(seq(11, 31, by = 2), hint = TRUE))

submission_1 %>% test_1 %>% test_2 
submission_2 %>% test_1 %>% test_2 
submission_3 %>% test_1 %>% test_2 
```

`Checkr` provides these matching functions:

* `match_number()` allows numerical comparison with a stated tolerance.
* `match_class()` checks that the class of the object is as specified.
* `match_data_frame()` provides a variety of tests on properties of a data frame.
* `match_formula()` checks symbolic formulas, e.g. `mpg ~ hp + carb`
* `match_vector()` provides tests for the equivalence of two vectors
* `agrees()` provides a way to make your own matching function from standard components.

Consider this exercise statement:

> Construct a linear model of `mpg` versus `hp` using `wt` as a covariate.

```{r}
submission_1 <- capture.code("lm(mpg ~ hp, data = mtcars)")  # wrong
submission_2 <- capture.code("lm(mpg ~ hp + wt, data = mtcars)") # right
submission_3 <- capture.code("lm(mpg ~ wt, data = mtcars)") # wrong
```

Depending on the kind of formative feedback you want to give, you might construct a sequence of tests, each of which looks for a specific problem in the submission.

```{r}
test_1 <- find_call("lm(data = mtcars)", "use lm() on mtcars data")
test_2 <- check_value(agrees('wt' %in% names(coef(x))), 
                      "what about the covariate wt?")
test_3 <- check_value(agrees(all(c("hp", "wt") %in% names(coef(x)))), 
                      'include both hp and the covariate as explanatory variables')
test_4 <- check_argument("lm(formula = grab_this)", match_formula(mpg ~ hp + wt))
```

```{r eval = FALSE}
submission_1 %>% test_1 %>% test_2 %>% test_3 
submission_2 %>% test_1 %>% test_2 %>% test_3
submission_3 %>% test_1 %>% test_2 %>% test_3 
submission_1 %>% test_1 %>% test_4 
submission_3 %>% test_1 %>% test_4 
```



## Creating and debugging tests

Note that in the examples, the location tests are all created and named *before* being used in the `USER_CODE %>% ...` checking line. Each test is implemented as an unevaluated function.

You might **incorrectly** write
```r
USER_CODE %>% find_call("whatever + whatever", "remember to use +")
```
This is not a check statement. Instead, the statement passes `USER_CODE` to the `find_call()` function. But `find_call()` doesn't take user code as an input. Rather, the first argument to `find_call()` should be the pattern to be matched. The output of `find_call()` is another function. That function *does* take user code as input.

In debugging a single test such as `test_a`, you should call `debug(test_a)`. This "functional programming" style has the potential to be confusing to many users.



# Matching values

Sometimes, all you want to do is check the output values produced by the submitted code against a `-solution` that you have provided. The `soln_test()` function in `checkr` makes this relatively easy. To illustrate, let's mimic the work done by `tutor` in handing off submitted code and `-solution` code to `checkr`.

```{r}
# these values are provided by tutor
USER_CODE   <- capture.code("2 + 2 + 2")
SOLN_CODE   <- capture.code("2 + 2")
```

Obviously, the result of the user code will not, in this example, match the output of the solution code. Here's a simple solution-matching `-check` statement.


```{r}
soln_test(USER_CODE, SOLN_CODE,
             res = final_,
             same_num(res)) 
```
The `soln_test()` function from `checkr` compares the values produced by the student's code and the values produced by code provided by the test writer in a `-solution` chunk. `USER_CODE` and `SOLN_CODE` convey the contents of the submission and solution chunks, respectively. 

The next argument to `soln_test()` uses a function `final_()`, which looks for the *last* value produced by the student and by the solution code. In this example, there's only one value, but in general there may be multiple values produced by submitted code. Whatever that last value be, and regardless of whether the student's or solution code assigned a name to the value, the argument name will be assigned to that value for future reference.

The final argument to `soln_test()` refers to the name `ref` created in the previous argument. The `same_num()` function compares the `ref` value in the student and the solution code. 

There are three `checkr` functions in the `same_` family:

* `same_num()` checks whether two numbers are the same
* `same_vec()` checks whether two vectors are the same.
* `same_()` allows you to provide your own comparison function.

At this writing, I am unsure whether to expand the number of `same_` functions or to provide comparison functions that can be used with `same_()` and to support authors writing their own comparison functions.  The comparison functions are themselves quite simple. See `checkr:::compare_length`, `checkr:::compare_numbers`, `checkr:::compare_vectors`, `checkr:::compare_classes`.

Comparison functions take three arguments (and, optionally, additional arguments in `...`). They are:

* `S` which will be the value from the solution code
* `R` which will be the value from the user's submission
* `hint` which is a logical signalling whether the give a diagnostic hint.

Comparison functions return an empty string (`""`) if the comparison passes, and a non-empty character string message (e.g., "has wrong length") if the comparison fails.

To illustrate:
```{r}
checkr:::compare_length
```

The argument `res = final_` in the above example locates a particular value in the user submission and in the solution code. The name, in this case `res`, can be used to refer to the two values in the comparison, e.g. `same_num(res)`. The reason to give a new value to the particular value is that the values may be unnamed in the submitted or solution code, or they may have different names. 

Use locator functions for the named arguments to `soln_test()`. You can use any of the `find_` set of functions -- e.g. `res = find_statement(regex="2 *\\+")`.

One flexible strategy for locating values is to assign a name to the desired value in the `-solution` chunk. Then use `closest_to()` as the locator function, giving, as an argument the unquoted name used in the solution.

For instance, suppose the (rather silly) problem statement is:

> Compute $\sqrt{14}$ and $7 + 4$.

The solution code might look like:
```{r}
SOLN_CODE <- capture.code("
  a <- sqrt(14)
  b <- 7 + 4")
```

Note that names have been assigned to the values being asked for.

**Note in DRAFT: fix `closest_to()` system.** Readers, ignore the rest of this section.

The student's submission might involve no names or extraneous names or misspelled names. For instance:
```{r}
USER_CODE <- capture.code("
  frist <- 4 + 7
  sqrt(14)")
```

Here's a simple solution-matching `-check` statement.


```{r eval = FALSE}
soln_test(USER_CODE, SOLN_CODE,
          one = closest_to(a),
          same_num(one, hint = TRUE),
          two = closest_to(b),
          same_num(b, hint = TRUE)) 
```


## Computing comparison features

You can compute on the named values. For instance, here we compare not the result itself but the absolute value of the result. Thus, the user code, which does not match the solution code, still passes the `same_num()` test, since the value $-4$ computed by the user passes the test.

```{r}
USER_CODE <- capture.code("2 + -6")
SOLN_CODE <- capture.code("2 + 2")
soln_test(USER_CODE, SOLN_CODE,
             res = find_statement("2 *\\+", regex = TRUE),
             same_num(abs(res))) 
```

The point of having such transformations -- e.g. `abs()` in the above example -- is to enable the test writer to compare complex objects like models using simple comparisons. For instance, suppose the user is asked to construct a model based on `mtcars`:
```{r eval = FALSE}
USER_CODE <- capture.code("mod <- lm(mpg ~ hp + carb, data = mtcars)")
SOLN_CODE <- capture.code("mod <- lm(mpg ~ hp * carb, data = mtcars)")
soln_test(USER_CODE, SOLN_CODE,
             res = find_assignment("mod"),
             same_vec(coef(res)))
```





# Pre-evaluation checking of user code

Before the locator and value tests can be applied to user code, that code must run successfully (even if it doesn't satisfy the instructions given in the exercise).

When the user code doesn't run, an R error will be displayed to the user. For many purposes, the tutorial author may regard this as perfectly satisfactory. But there may be circumstances where the author would like to intercept such errors in order to display a gentler or more helpful message.

By creating a chunk whose label has the extension `-code-check`, you can set up `checkr` tests that don't rely on the code being syntactically correct or otherwise not producing errors. For instance, for the `example-0` exercises, you can add a chunk like this:

    ```{r, example-0-code-check, exercise = TRUE}
    USER_CODE %>% check_blanks()
    USER_CODE %>% check_assignment_names()
    USER_CODE %>% check_function_calls("sqrt") 
    ```

This particular set of instructions checks whether names used in assignment are formed legally and whether `sqrt` is used as a function (if it is used at all). Being "used as a function" simply means that there are open and closed parentheses. In the future, additional checks may be added.

Notice that the pre-evaluation checking uses a different set of functions than the locator and value tests. That's because locator and value tests work on the assumption that the code being given them will evaluate properly. 

# Magrittr pipelines

Magrittr pipelines are translated into a sequence of commands. For instance,
```
foobar <- mtcars %>% filter(mpg > 15) %>% select(mpg)
```
will be evaluated as if it were
```
filter(mtcars, mpg > 15) -> ..tmp1..
select(..tmp1.., mpg) -> foobar
```

This allows tests to be constructed as if each step in the chain were a separate command; the test statements can be constructed in the ordinary way. Do remember that the first argument to `dplyr` data verbs is `.data =`.


```{r}
library(dplyr)
submission_1 <- capture.code("foobar <- mtcars %>% filter(mpg > 15)")
test_1 <- find_call("filter()", "should call filter()")
# Check whether the right value is used in filter
test_2 <- check_argument("mpg > grab_this", match_number(15))
# A test that will fail
test_2A <- check_argument("mpg > grab_this", match_number(16))
test_3 <- check_argument("filter(.data = grab_this)", match_data_frame(mtcars))
# A test that will fail
test_3A <- check_argument("filter(.data = grab_this)", match_data_frame(iris, hint = TRUE))
submission_1 %>% test_1 %>% test_2 %>% test_3 # should pass
submission_1 %>% test_1 %>% test_2A # should fail
submission_1 %>% test_1 %>% test_2 %>% test_3A # should fail

```


```{r}
submission_2 <- capture.code("
  mtcars %>% 
    filter(mpg > 15) %>% 
    group_by(cyl) %>% 
    summarise(mmpg = mean(mpg))")
test_4 <- find_call("group_by()", "remember to group before summarizing.")
test_5 <- find_call("group_by(.data = whatever, cyl)", "break down the result by `cyl`")
test_6 <- find_call("summarise()", "use summarise() after grouping")
test_7 <- find_call("mean(mpg)", "calculate the mean of mpg for each group")
submission_2 %>% test_4 %>% test_5 %>%  then %>% test_6 %>% test_7
```

In contrast, this chain has steps that are individually value, but in the wrong order. 

```{r}
submission_2 <- capture.code("
  mtcars %>% 
    filter(mpg > 15) %>% 
    summarise(mmpg = mean(mpg), cyl = mean(cyl)) %>%
    group_by(cyl)")

submission_2 %>% test_4 %>% test_5 %>%  then %>% test_6 %>% test_7
```

I'm exploring whether it's helpful to add pipe-specific tests that make it easy to check the order of statements.


# Logging and authentication

## Setting up logging via Google Sheets

Currently, the only logging system implemented is via Google Sheets. (This can be used on any Shiny server, as it does *not* require persistent storage on the server.) Using Google Sheets has the advantage that any instructor can set up a sheet and access it using ordinary tools. Thus, the instructor can maintain confidentiality and security.

### Creating a sheet

1. Create a Google Sheet using the usual interface. Give it whatever name you like, perhaps "STAT_325_submissions". You can set the sharing however you like to give access to the appropriate people. It does not need to be publicly accessible. Initialize the file by entering these values in the first two rows of the first column: "submission", "bogus".
2. Each tutorial consists of an Rmd file. That file lives in some directory on your development machine. **One time only**, go to that directory and run `checkr::set_up_google_auth_token()`. This will bring up web pages for Google authentification. It will create two files, `.httr-oauth` and `checkr_app_token.rds`. These files must live in the directory of the .Rmd file. 

> Security note: The `.httr-oauth` file gives access to all the sheets in the Google account used in the authentication process. You therefore want to keep it confidential. Some suggestions:    
>    * Set up a Google account specifically for the purpose of logging.    

>    * If you use publicly available version control (e.g. GitHub) make sure to add the files to your `.gitignore` file as soon as they are created. That is, make sure the `.httr-oauth` and `checkr_app_token.rds` files are not visible.

>    * Put your tutorial on a server (e.g. `shinyapps.io`) where the student/user cannot access the source files.

>    * If you want to distribute your tutorials in source form, do not include the `.httr-oauth` or `checkr_app_token.rds` files with the source. The person who uses your source materials can set up his or her own Google sheet for logging.
        
3. In the start-up chunk for each tutorial, put the following command:

```r
turn_on_google_logging("your_spreadsheet_title")
```

4. When you upload the files for the tutorial to a Shiny server, include the `.httr-oauth` and `checkr_app_token.rds` files alongside the tutorial Rmd file. You do *not* need to re-authenticate on the server (nor could you on servers like `shinyapps.io`).

-----------------------




> NOTE: This system is under development. The current version simply takes the entered ID at face value and checks whether the password is literally `pass`. Later versions will take the name of a file containing user-ID/password pairs.




<!-- We need to settle on a way to have persistent storage on `shinyapps.io`. Right now, this will work only on a user's machine. It still needs to be tested even on a regular shiny server. -->

Each time an exercise submission is made, `checkr` logs information about the submission: user ID (if any), time stamp, user code, the message returned in response by `checkr`, and whether the submission was deemed correct. The log-file, `"log-test.txt"`, will eventually be made to live in the same directory as the Rmd file containing the tutorial. It is a flat, plain text file, with each submission recorded in JSON format as a list.

If you are using the tutorial as part of a course, you may wish to record the student's submissions for scoring. Or, if you are writing a tutorial, you might want that record in order to see what kind of code submissions students have made, so that you can write better tests for them.

If you want to record the user's ID along with the other submission data, add this chunk, verbatim, to the tutor Rmd document.

    ```{r child = checkr::authentication()}
    ```

This will display text-entry fields for the student to enter an ID and password.




